import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def is_valid(url):
    """
    Checks whether `url` is a valid URL.
    """
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def get_all_website_links(url):
    """
    Returns all URLs that are found on `url` in which it belongs to the same website.
    """
    urls = set()
    domain_name = urlparse(url).netloc
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    for a_tag in soup.findAll("a"):
        href = a_tag.attrs.get("href")
        if href == "" or href is None:
            # href empty tag
            continue
        # join the URL if it's relative (not absolute link)
        href = urljoin(url, href)
        parsed_href = urlparse(href)
        # remove URL GET parameters, URL fragments, etc.
        href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path
        if not is_valid(href):
            # not a valid URL
            continue
        if domain_name not in href:
            # external link
            continue
        urls.add(href)
    return urls

def crawl(url):
    """
    Crawls a website starting from the `url`.
    """
    visited_urls = set()
    urls_to_visit = set([url])
    
    while urls_to_visit:
        current_url = urls_to_visit.pop()
        if current_url in visited_urls:
            continue
        print(f"Crawling: {current_url}")
        visited_urls.add(current_url)
        new_links = get_all_website_links(current_url)
        urls_to_visit = urls_to_visit.union(new_links - visited_urls)
    
    return visited_urls

if __name__ == "__main__":
    start_url = input("Enter the URL to start crawling: ")
    all_links = crawl(start_url)
    print(f"Total links found: {len(all_links)}")
    for link in all_links:
        print(link)
